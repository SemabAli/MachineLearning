Data programming through labeling functions generating training sets from data


***Basic MAchine Learning Models:

1) Linear (linear regression and SVM)   (if data is linearly separated)

2) Tree based  (Decision tree and random forest, cat boost, gradient boosting--very powerful----predictions of the individual trees are summed)  (Linear relationship is hard to capture)  (Divide and conquer approach can wrk with non-linear data as well)

3) KNN (need feature scaling as distance calculated)(---idea behind it closer objects have same labels)

4) Neural networks


***Predictive modeling vs Descriptive
Predictive models use known results to develop (or train) a model that can be used to predict values for different or new data.
In predictive modeling, you are essentially trying to create a function closer 
to original function/process which generated the data you have

Descriptive modeling or clustering is a technique that involves the grouping of data points.
**Descriptive analytics is the analysis of past (or historical) data to understand trends Descriptive analytics is limited to 
representing data in the tabular and graphical form. e-g tableau

data points that are in the same group should have similar properties and/or features

***k means-clustering
SElect no of clusters group classes and randomly intialized them
calculate distance each data point to these points and assigned them group
claculate mean afterwards/shift centeroid
stop when centeroid not have enough movemnt between iteration.

******Feature Encoding

1)Tree based 
In tree based label encoder or factorize can be used tbut dummtencoding is wastage of efoort. [XGBoost and CATBOOST can handle NA VAlues.]

2)Non tree based model
In non tree neural networks,linear regression dummy encoding as weell as feature scaling[0-1 minmaxscalar] is necessary also for K_Nearest_Neighbour.

**Checking null values check by plotting histogram if one value say -1 has a tall bar from rest of data indicating outlier. 

**use Label encoder(numeric encoding)  with tree based models as if(male==1) not matters but not with regression as weitage increses one hot-encoding is best with non tree based models


**************************************Bagging Boosting**********************************************

DECISION TREE REGREESOR RANDOM FOREST REGREESOR for regression sklearn proviedes

 general rule in machine learning is that the more features you have, 
 the more likely your model will suffer from overfitting and vice versa.
 Random forest select sunset of features
 
Ensemble methods:
Predict the output (regression or classification) by combining the output of various individual trees.
Bagging and random forest both are ensemble methods

1) Averging
2) stacking
        all of the other algorithms are trained using the available data, then a combiner algorithm is 
        trained to make a final prediction using all the predictions of the other algorithms as additional inputs.
        neural networks with svm,logistic regression
3) Bagging
4) Boosting

Bagging:
multiple decision trees built parallel using subset of observation
about one-third of the data is not used to train the model and can be used to evaluate its performance.
These samples are called the out of bag samples. 
Each tree is constructed using a different bootstrap sample from the original data. 
About one-third of the cases are left out of the bootstrap sample .


Random forest:
Builts trees parallely and independtly
Random Forest algorithm randomly selects observations and features to build several decision trees and then averages the results.
is a kind of bagging alogorthm
No need of separet testing as out of bag sample used for tesing
* used for both classification and regression.
* give feature importance
* more robust to overfitting than decision trees
* The real world data is noisy and contains many missing values
* perfect for multi class classification
* easy to finetune 2 fetures(No of trees and no of features selected at each node)
* large trees may slow for real time predictions


Boosting:
only one tree at a time
Previous trained tree help to correct error in new tree
* sensitive to overfitting data is noisy
* large traing time as trees built sequentially




*** oversampling
!) adding examples similar to examplles which have low class labels to balanced data

*** calculating corelation and covarience
1) peearson to calculate non categorical variable
2) chi sqaure test for categorical variables
3) how one variable change effect other


*** vanishing gradient 
1) small change in updation of weights 
2) sigmoid maps large input space to 0.1 range and then maps to more shorter output space if multiple layers network have


** Sckit multilearn
1) for multilabel classification
2) use humming loss for multilabel

** CNN have unblanced data

1) class weight function if class0 is 20 times more frequent then set class1 weight to 20times more so that penalty on each is same.
2) So higher class-weight means you want to put more emphasis on a class.
3) small kernel/filter size i.e (1,3) complex features are extrracted same with case of no of filters
4) 
  ?, 20,20,1600
  20 width 20 height 1600 channels,filters
  ?,1,1,4
  apply global average pooling and apply 4 filters

5) Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last')
  kernel size is odd numbers:{1, 3, 5, 7} mostly. beacuse applyying filter one stride value is replaced at center in case of 2X2 no center 
  i.e 2 0 4    equal to  ?   ?   ?   question mark are filled with padding parametrer.
      3 6 9              ?  val  ?
      5 9 0              ?   ?    ?

 in case of max pooling 2X2 can be used as 2 0 4   returns [9] placed at first pixel 
                                           6 0 7
                                           7 9 0


*** R2 ( Coefficient of determination )
1) formation about the goodness of fit of a model
1)  An R2 of 1 indicates that the regression predictions perfectly fit the data
2) always increases adding variuable



** financial forcasting
1) use catboost and random forest paraale and apply linear regression on output of these
2) random forest works well if less outliers in your data

**MUltilabel vs multiclass classification
1) at a time one class cat or dog or rat
2) at at time two classes possible one pic conatians cat and dog labels =[1,1,0]


** No of fileters and filter size
1) increasing no of filteres increased prediction probablity
2) decresing kernel size learns complex features
3) dropout is like each student answer question rest are quiet it enables rest of nodes to learn same features by deactivating some randonmly.

**Vanilla rnn vs LStM

Vanilla rnn ha vanishing gradient problem and could not able to learn long sequences i.e the cat which comes to our home was red in color  the cats that come to our home yesterday were in red incolor.
to remember "was or were" vanilla rnn gradient vanishes

exploding gradient can be coreected by gradient clipping whle vanishing gradient is hard to solve

doesn't share features learned across different position of text.activation passing with each timestamp in RNN
beteen layers
parameters sharing input layer to hidden layers


****network architecture:

----channel first
Image (3 x 224 x 224)) ->
a convolutional part of VGG-16 that returns (512 x 7 x 7) ->
your new conv2d layer (2 filters) (2 x 7 x 7) ->
Global Max /Average Pooling layer (2 x 1 x 1) ->
Softmax (2)


** Object Detectors
1) Faster Rcnn (nas feature extractor)
2) SSD Single shot detector
for real time video 
3) Yolo

***   Architectures of CNN (Feature Extracter):
Alexnet,VGG,Resnet,Inception/Googlenet,Nasnet

*** Different Object Detectors:
Faster RCNN,SSD , YOLO, Retinanet

**how differnt:
1) feature extracter
2) Box encoding
3) Roi Proposal system
4) loss function
5) input image resoultion


*** OCR
for small documents
use cptian model(text bounding box predi) + tesseract e-g CNIC
allign library uses text direction to allign the documents
