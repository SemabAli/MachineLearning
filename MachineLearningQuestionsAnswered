Data programming through labeling functions generating training sets from data


***Basic MAchine Learning Models:

1) Linear (linear regression and SVM)   (if data is linearly separated)

2) Tree based  (Decision tree and random forest, cat boost, gradient boosting--very powerful----predictions of the individual trees are summed)  (Linear relationship is hard to capture)  (Divide and conquer approach can wrk with non-linear data as well)

3) KNN (need feature scaling as distance calculated)(---idea behind it closer objects have same labels)

4) Neural networks


******Feature Encoding

1)Tree based 
In tree based label encoder or factorize can be used tbut dummtencoding is wastage of efoort. [XGBoost and CATBOOST can handle NA VAlues.]

2)Non tree based model
In non tree neural networks,linear regression dummy encoding as weell as feature scaling[0-1 minmaxscalar] is necessary also for K_Nearest_Neighbour.

**Checking null values check by plotting histogram if one value say -1 has a tall bar from rest of data indicating outlier. 

**use Label encoder(numeric encoding)  with tree based models as if(male==1) not matters but not with regression as weitage increses one hot-encoding is best with non tree based models


*** oversampling
!) adding examples similar to examplles which have low class labels to balanced data

*** calculating corelation and covarience
1) peearson to calculate non categorical variable
2) chi sqaure test for categorical variables
3) how one variable change effect other


*** vanishing gradient 
1) small change in updation of weights 
2) sigmoid maps large input space to 0.1 range and then maps to more shorter output space if multiple layers network have


** Sckit multilearn
1) for multilabel classification
2) use humming loss for multilabel

** CNN have unblanced data

1) class weight function if class0 is 20 times more frequent then set class1 weight to 20times more so that penalty on each is same.
2) So higher class-weight means you want to put more emphasis on a class.
3) large kernel size complex features are extrracted same with case of no of filters

*** R2 ( Coefficient of determination )
1) formation about the goodness of fit of a model
1)  An R2 of 1 indicates that the regression predictions perfectly fit the data
2) always increases adding variuable



** financial forcasting
1) use catboost and random forest paraale and apply linear regression on output of these
2) random forest works well if less outliers in your data

**MUltilabel vs multiclass classification
1) at a time one class cat or dog or rat
2) at at time two classes possible one pic conatians cat and dog labels =[1,1,0]


** No of fileters and filter size
1) increasing no of filteres increased prediction probablity
2) decresing kernel size learns complex features
3) dropout is like each student answer question rest are quiet it enables rest of nodes to learn same features by deactivating some randonmly.
